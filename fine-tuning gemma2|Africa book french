{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8905636,"sourceType":"datasetVersion","datasetId":5354410}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-18T21:09:42.250067Z","iopub.execute_input":"2024-09-18T21:09:42.250591Z","iopub.status.idle":"2024-09-18T21:09:42.621855Z","shell.execute_reply.started":"2024-09-18T21:09:42.250555Z","shell.execute_reply":"2024-09-18T21:09:42.620998Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llm-books-africa/book/Pinguilly_Yves Contes_et_légendes_L Afrique_d ouest_en_est.pdf\n/kaggle/input/llm-books-africa/book/L enfant Noir Camara Laye.pdf\n/kaggle/input/llm-books-africa/book/La victoire des vaincus Jean Ziegler.pdf\n/kaggle/input/llm-books-africa/book/Pinguilly Yves Contes_et_légendes_La_Corne_de_l Afrique.pdf\n/kaggle/input/llm-books-africa/book/Le Mvett III.pdf\n/kaggle/input/llm-books-africa/book/Les Soleils des independances Ahmadou Kourouma.pdf\n/kaggle/input/llm-books-africa/book/Le_fabuleux_et_triste_destin_d’Ivan_et_d’Ivana_Condé_Maryse.pdf\n/kaggle/input/llm-books-africa/book/Condé Maryse Conte cruel.pdf\n/kaggle/input/llm-books-africa/book/La belle Créole - Condé Maryse.pdf\n/kaggle/input/llm-books-africa/book/Le pagne noir contes africains.pdf\n/kaggle/input/llm-books-africa/book/Vallerey_Gisèle Contes_et_Legendes_de_lAfrique_Noire.pdf\n/kaggle/input/llm-books-africa/book/L étrange destin de wangrin  Hamadou Hampaté Bâ.pdf\n/kaggle/input/llm-books-africa/book/Rêves amers - Condé Maryse.pdf\n/kaggle/input/llm-books-africa/book/Victoire, les saveurs et les mots - Condé Maryse.pdf\n/kaggle/input/llm-books-africa/book/Kourouma Ahmadou Quand on refuse on dit non.pdf\n/kaggle/input/llm-books-africa/book/Cheikh Hamidou Kane  L aventure ambigue.pdf\n/kaggle/input/llm-books-africa/book/Camara Laye Dramouss.pdf\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:09:42.623469Z","iopub.execute_input":"2024-09-18T21:09:42.623860Z","iopub.status.idle":"2024-09-18T21:09:57.059974Z","shell.execute_reply.started":"2024-09-18T21:09:42.623824Z","shell.execute_reply":"2024-09-18T21:09:57.058871Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting nvidia-smi\n  Downloading nvidia_smi-0.1.3-py36-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (1.26.4)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (1.16.0)\nCollecting sorcery>=0.1.0 (from nvidia-smi)\n  Downloading sorcery-0.2.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: pytest>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (8.3.2)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (2.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (21.3)\nRequirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (1.5.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (1.2.0)\nRequirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (2.0.1)\nRequirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (2.0.1)\nCollecting littleutils>=0.2.1 (from sorcery>=0.1.0->nvidia-smi)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (2.4.1)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pytest>=4.3.1->nvidia-smi) (3.1.2)\nDownloading nvidia_smi-0.1.3-py36-none-any.whl (11 kB)\nDownloading sorcery-0.2.2-py3-none-any.whl (16 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, sorcery, nvidia-smi\nSuccessfully installed littleutils-0.2.4 nvidia-smi-0.1.3 sorcery-0.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U bitsandbytes ","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:09:57.061395Z","iopub.execute_input":"2024-09-18T21:09:57.061720Z","iopub.status.idle":"2024-09-18T21:10:14.307949Z","shell.execute_reply.started":"2024-09-18T21:09:57.061683Z","shell.execute_reply":"2024-09-18T21:10:14.306910Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U peft ","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:10:14.310895Z","iopub.execute_input":"2024-09-18T21:10:14.311676Z","iopub.status.idle":"2024-09-18T21:10:27.512082Z","shell.execute_reply.started":"2024-09-18T21:10:14.311635Z","shell.execute_reply":"2024-09-18T21:10:27.510932Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.44.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.33.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.4)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.12.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U trl ","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:10:27.513702Z","iopub.execute_input":"2024-09-18T21:10:27.514059Z","iopub.status.idle":"2024-09-18T21:10:41.469342Z","shell.execute_reply.started":"2024-09-18T21:10:27.514021Z","shell.execute_reply":"2024-09-18T21:10:41.468175Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting trl\n  Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.4.0)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.44.0)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.33.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.21.0)\nCollecting tyro>=0.5.11 (from trl)\n  Downloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.24.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl) (4.66.4)\nRequirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.1)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.10.1-py3-none-any.whl (280 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.11-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, tyro, trl\nSuccessfully installed shtab-1.7.1 trl-0.10.1 tyro-0.8.11\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U accelerate","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:10:41.470700Z","iopub.execute_input":"2024-09-18T21:10:41.471027Z","iopub.status.idle":"2024-09-18T21:10:56.388427Z","shell.execute_reply.started":"2024-09-18T21:10:41.470991Z","shell.execute_reply":"2024-09-18T21:10:56.386913Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\nCollecting accelerate\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.33.0\n    Uninstalling accelerate-0.33.0:\n      Successfully uninstalled accelerate-0.33.0\nSuccessfully installed accelerate-0.34.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:10:56.389938Z","iopub.execute_input":"2024-09-18T21:10:56.390285Z","iopub.status.idle":"2024-09-18T21:11:20.054427Z","shell.execute_reply.started":"2024-09-18T21:10:56.390246Z","shell.execute_reply":"2024-09-18T21:11:20.053289Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\nSuccessfully installed transformers-4.44.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install pyPDF2","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:11:20.055796Z","iopub.execute_input":"2024-09-18T21:11:20.056105Z","iopub.status.idle":"2024-09-18T21:11:33.369528Z","shell.execute_reply.started":"2024-09-18T21:11:20.056070Z","shell.execute_reply":"2024-09-18T21:11:33.368427Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting pyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyPDF2\nSuccessfully installed pyPDF2-3.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip -U install peftModel","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:11:33.370999Z","iopub.execute_input":"2024-09-18T21:11:33.371364Z","iopub.status.idle":"2024-09-18T21:11:34.751610Z","shell.execute_reply.started":"2024-09-18T21:11:33.371326Z","shell.execute_reply":"2024-09-18T21:11:34.750298Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\nUsage:   \n  /opt/conda/bin/python3.10 -m pip <command> [options]\n\nno such option: -U\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# import llibraries ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling, BitsAndBytesConfig\nfrom huggingface_hub import notebook_login\nimport torch\nimport os\nfrom PyPDF2 import PdfReader\nimport locale\nimport re\nfrom nltk.tokenize import word_tokenize\nimport nltk\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom datasets import Dataset, DatasetDict\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:11:42.507753Z","iopub.execute_input":"2024-09-18T21:11:42.508139Z","iopub.status.idle":"2024-09-18T21:12:02.323599Z","shell.execute_reply.started":"2024-09-18T21:11:42.508099Z","shell.execute_reply":"2024-09-18T21:12:02.322775Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Télécharger les ressources nécessaires de NLTK\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:12:02.325131Z","iopub.execute_input":"2024-09-18T21:12:02.325761Z","iopub.status.idle":"2024-09-18T21:12:02.483138Z","shell.execute_reply.started":"2024-09-18T21:12:02.325725Z","shell.execute_reply":"2024-09-18T21:12:02.482237Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# Huggingface login ","metadata":{}},{"cell_type":"code","source":"# fixing unicode error in google colab\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n#Make sure to create a huggingface account and generate your token to login with here.\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:12:06.717884Z","iopub.execute_input":"2024-09-18T21:12:06.718634Z","iopub.status.idle":"2024-09-18T21:12:06.742832Z","shell.execute_reply.started":"2024-09-18T21:12:06.718592Z","shell.execute_reply":"2024-09-18T21:12:06.741932Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60479fc72f141ba9f1ade393623a094"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"# Load and initialize the model \nmodel_name = \"google/gemma-2-9b-it\"\n#gemma2_model = \"google/gemma-2-27b-it\"\n#gemma2_model = \"google/gemma-7b-it\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:12:16.606081Z","iopub.execute_input":"2024-09-18T21:12:16.606517Z","iopub.status.idle":"2024-09-18T21:12:16.611360Z","shell.execute_reply.started":"2024-09-18T21:12:16.606478Z","shell.execute_reply":"2024-09-18T21:12:16.610064Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def load_quantized_model(model_name: str):\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,                   #This tells the system to load the model using 4-bit precision, which drastically reduces memory usage.\n        bnb_4bit_use_double_quant=True,      #Enables double quantization, a technique used to further compress the model while retaining accuracy.\n        bnb_4bit_quant_type=\"nf4\",           #his specifies the type of quantization as NF4, which is a particular type of 4-bit quantization technique used in machine learning.\n        bnb_4bit_compute_dtype=torch.bfloat16   #This sets the computation to use the bfloat16 (16-bit floating point) data type, which is less precise than 32-bit but much faster and consumes less memory, making it efficient for large models.\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        quantization_config=bnb_config\n    )\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:12:17.630853Z","iopub.execute_input":"2024-09-18T21:12:17.631244Z","iopub.status.idle":"2024-09-18T21:12:17.637112Z","shell.execute_reply.started":"2024-09-18T21:12:17.631206Z","shell.execute_reply":"2024-09-18T21:12:17.636169Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def initialize_tokenizer(model_name: str):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.bos_token_id = tokenizer.convert_tokens_to_ids('<s>')  # Exemples de configuration\n    return tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:12:19.160853Z","iopub.execute_input":"2024-09-18T21:12:19.161464Z","iopub.status.idle":"2024-09-18T21:12:19.166067Z","shell.execute_reply.started":"2024-09-18T21:12:19.161423Z","shell.execute_reply":"2024-09-18T21:12:19.165012Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = load_quantized_model(model_name)\ntokenizer = initialize_tokenizer(model_name)\n# Spécifier les IDs des tokens d'arrêt\nstop_token_ids = [tokenizer.eos_token_id]","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:12:20.450453Z","iopub.execute_input":"2024-09-18T21:12:20.451307Z","iopub.status.idle":"2024-09-18T21:15:11.951625Z","shell.execute_reply.started":"2024-09-18T21:12:20.451267Z","shell.execute_reply":"2024-09-18T21:15:11.950774Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aea3350695f43fcba94b74aaa98fa28"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93024fb55adc403b945297c97370e50d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5d7758e4f242e582ba291ee65a176e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcbad5260de9492387471977a175a0f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eece9c6bdf543a489678b6d98e115b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b2c723f28c4978ac2fdb17efe8d038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e0ee3f9f49241bd8b91d8183108e80f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef932adf248412db406d1dfbd0a8224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10fd754e26ac4d0fae19fdb86aa7ebbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af219ac59fde4dc3aeff23d858b10534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f868725ed047411b97997fc2594d38d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"432bbef77906434188c4169cbd87db76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccacbb629b2b4323886dcddfe46c736c"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"# Répertoire contenant les fichiers PDF\npdf_dir = \"/kaggle/input/llm-books-africa/book\"\n\ndata = []\n\n# Parcourir les fichiers PDF et extraire les informations\nfor filename in os.listdir(pdf_dir):\n    if filename.endswith(\".pdf\"):\n        file_path = os.path.join(pdf_dir, filename)\n        reader = PdfReader(file_path)\n        \n        # Extraire le contenu (texte) et le nombre de pages\n        content = \"\"\n        for page in reader.pages:\n            content += page.extract_text()\n        num_pages = len(reader.pages)\n        \n        # Ajouter les informations extraites dans la liste de données\n        data.append({\"filename\": filename, \"content\": content, \"page\": num_pages})\n\n# Créer un dataset Hugging Face à partir des données extraites\ndataset = Dataset.from_list(data)\n\n# Vérifier la structure du dataset\nprint(dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:15:27.810862Z","iopub.execute_input":"2024-09-18T21:15:27.811789Z","iopub.status.idle":"2024-09-18T21:18:57.654296Z","shell.execute_reply.started":"2024-09-18T21:15:27.811746Z","shell.execute_reply":"2024-09-18T21:18:57.653335Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['filename', 'content', 'page'],\n    num_rows: 17\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Lora Config","metadata":{}},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:19:41.206098Z","iopub.execute_input":"2024-09-18T21:19:41.206535Z","iopub.status.idle":"2024-09-18T21:19:41.242690Z","shell.execute_reply.started":"2024-09-18T21:19:41.206496Z","shell.execute_reply":"2024-09-18T21:19:41.241922Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:19:42.010736Z","iopub.execute_input":"2024-09-18T21:19:42.011891Z","iopub.status.idle":"2024-09-18T21:19:42.022180Z","shell.execute_reply.started":"2024-09-18T21:19:42.011848Z","shell.execute_reply":"2024-09-18T21:19:42.021363Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Gemma2ForCausalLM(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n    (layers): ModuleList(\n      (0-41): 42 x Gemma2DecoderLayer(\n        (self_attn): Gemma2SdpaAttention(\n          (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n          (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n      )\n    )\n    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n  )\n  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import bitsandbytes as bnb\ndef find_all_linear_name(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n        if 'lm_head' in lora_module_names: \n            lora_module_names.remove('lm_head')\n    return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:19:42.610644Z","iopub.execute_input":"2024-09-18T21:19:42.611029Z","iopub.status.idle":"2024-09-18T21:19:42.617585Z","shell.execute_reply.started":"2024-09-18T21:19:42.610991Z","shell.execute_reply":"2024-09-18T21:19:42.616664Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"module = find_all_linear_name(model)\nprint(module)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:19:45.066312Z","iopub.execute_input":"2024-09-18T21:19:45.066712Z","iopub.status.idle":"2024-09-18T21:19:45.073720Z","shell.execute_reply.started":"2024-09-18T21:19:45.066676Z","shell.execute_reply":"2024-09-18T21:19:45.072570Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"['k_proj', 'q_proj', 'gate_proj', 'down_proj', 'v_proj', 'o_proj', 'up_proj']\n","output_type":"stream"}]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r = 64,\n    lora_alpha=32,\n    target_modules=module,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:19:47.436308Z","iopub.execute_input":"2024-09-18T21:19:47.436706Z","iopub.status.idle":"2024-09-18T21:19:50.314427Z","shell.execute_reply.started":"2024-09-18T21:19:47.436669Z","shell.execute_reply":"2024-09-18T21:19:50.313541Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainable, total = model.get_nb_trainable_parameters()\nprint(f\"trainable : {trainable} | Total : {total} | Percentage : {trainable/total*100:.4f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:19:55.959797Z","iopub.execute_input":"2024-09-18T21:19:55.960210Z","iopub.status.idle":"2024-09-18T21:19:55.980133Z","shell.execute_reply.started":"2024-09-18T21:19:55.960149Z","shell.execute_reply":"2024-09-18T21:19:55.979025Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"trainable : 216072192 | Total : 9457778176 | Percentage : 2.2846%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Run training fine-tuning","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    eval_dataset = dataset,\n    dataset_text_field=\"content\",\n    peft_config=lora_config,\n    args = TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=0,\n        #num_train_epochs=1,\n        max_steps=10,\n        learning_rate=2e-4,\n        logging_steps=1,\n        output_dir=\"output\",\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n        #report_to = \"wandb\"\n    ),\n    \n\ndata_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:20:00.542253Z","iopub.execute_input":"2024-09-18T21:20:00.542933Z","iopub.status.idle":"2024-09-18T21:20:07.544098Z","shell.execute_reply.started":"2024-09-18T21:20:00.542892Z","shell.execute_reply":"2024-09-18T21:20:07.543278Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0ec372dbdd14199b447435a5acc2783"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8969319dc0994233a06e2ba856294ef6"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:20:07.545978Z","iopub.execute_input":"2024-09-18T21:20:07.546379Z","iopub.status.idle":"2024-09-18T21:28:37.631885Z","shell.execute_reply.started":"2024-09-18T21:20:07.546332Z","shell.execute_reply":"2024-09-18T21:28:37.630892Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240918_212116-v72laaml</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ipoukmarievictoire1-national-advanced-school-of-engineering/huggingface/runs/v72laaml' target=\"_blank\">output</a></strong> to <a href='https://wandb.ai/ipoukmarievictoire1-national-advanced-school-of-engineering/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ipoukmarievictoire1-national-advanced-school-of-engineering/huggingface' target=\"_blank\">https://wandb.ai/ipoukmarievictoire1-national-advanced-school-of-engineering/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ipoukmarievictoire1-national-advanced-school-of-engineering/huggingface/runs/v72laaml' target=\"_blank\">https://wandb.ai/ipoukmarievictoire1-national-advanced-school-of-engineering/huggingface/runs/v72laaml</a>"},"metadata":{}},{"name":"stderr","text":"It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 06:29, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.131300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.092100</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.970800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.507000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.004300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.561700</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.993400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.999700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.067200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.385300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=1.9712850093841552, metrics={'train_runtime': 508.3168, 'train_samples_per_second': 0.079, 'train_steps_per_second': 0.02, 'total_flos': 1941597172816896.0, 'train_loss': 1.9712850093841552, 'epoch': 2.3529411764705883})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Quantizier post trainer","metadata":{}},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\ntorch.cuda.empty_cache()\ntorch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:29:13.581455Z","iopub.execute_input":"2024-09-18T21:29:13.581864Z","iopub.status.idle":"2024-09-18T21:29:13.807848Z","shell.execute_reply.started":"2024-09-18T21:29:13.581826Z","shell.execute_reply":"2024-09-18T21:29:13.806823Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   8439 MiB |  13902 MiB |  12864 GiB |  12856 GiB |\\n|       from large pool |   7926 MiB |  13007 MiB |  12813 GiB |  12805 GiB |\\n|       from small pool |    512 MiB |    901 MiB |     51 GiB |     50 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   8439 MiB |  13902 MiB |  12864 GiB |  12856 GiB |\\n|       from large pool |   7926 MiB |  13007 MiB |  12813 GiB |  12805 GiB |\\n|       from small pool |    512 MiB |    901 MiB |     51 GiB |     50 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   8438 MiB |  13879 MiB |  12843 GiB |  12835 GiB |\\n|       from large pool |   7926 MiB |  12984 MiB |  12792 GiB |  12784 GiB |\\n|       from small pool |    512 MiB |    900 MiB |     51 GiB |     50 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  10002 MiB |  15420 MiB |  19882 MiB |   9880 MiB |\\n|       from large pool |   9446 MiB |  14446 MiB |  18874 MiB |   9428 MiB |\\n|       from small pool |    556 MiB |    974 MiB |   1008 MiB |    452 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   1562 MiB |   3497 MiB |  13079 GiB |  13077 GiB |\\n|       from large pool |   1519 MiB |   3455 MiB |  13024 GiB |  13023 GiB |\\n|       from small pool |     43 MiB |     76 MiB |     54 GiB |     54 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    3451    |    4637    |  699472    |  696021    |\\n|       from large pool |     423    |     624    |  473231    |  472808    |\\n|       from small pool |    3028    |    4088    |  226241    |  223213    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    3451    |    4637    |  699472    |  696021    |\\n|       from large pool |     423    |     624    |  473231    |  472808    |\\n|       from small pool |    3028    |    4088    |  226241    |  223213    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     443    |     657    |     728    |     285    |\\n|       from large pool |     165    |     170    |     224    |      59    |\\n|       from small pool |     278    |     487    |     504    |     226    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     302    |     646    |  350246    |  349944    |\\n|       from large pool |     124    |     168    |  248761    |  248637    |\\n|       from small pool |     178    |     478    |  101485    |  101307    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Évaluation du modèle\n#eval_results = trainer.evaluate()\n#print(f\"Résultats de l'évaluation : {eval_results}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:30:04.945664Z","iopub.execute_input":"2024-09-18T21:30:04.946825Z","iopub.status.idle":"2024-09-18T21:30:04.952100Z","shell.execute_reply.started":"2024-09-18T21:30:04.946777Z","shell.execute_reply":"2024-09-18T21:30:04.950892Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"code","source":"from transformers import GenerationConfig\n\ndef generate_text(prompt, model, tokenizer, max_length=200, num_return_sequences=1, temperature=0.7, top_k=50, top_p=0.95):\n    # Tokenize the input and move to the same device as the model\n    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n    \n    # Cast the input tensor to LongTensor to match the expected input type\n    inputs = inputs.long()\n    \n    # Prepare generation configuration\n    generation_config = GenerationConfig(\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    \n    # Generate text with the model\n    generated_outputs = model.generate(\n        inputs,\n        generation_config=generation_config,\n        use_cache=True,\n    )\n    \n    # Decode generated outputs\n    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in generated_outputs]\n    return generated_texts","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:50:37.813985Z","iopub.execute_input":"2024-09-18T21:50:37.815067Z","iopub.status.idle":"2024-09-18T21:50:37.824765Z","shell.execute_reply.started":"2024-09-18T21:50:37.815019Z","shell.execute_reply":"2024-09-18T21:50:37.823652Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Example usage of the generate_text function\nprompt = \"Il était une fois une fille qui lavait un pagne dans la riviere des l'aube\"\ngenerated_texts = generate_text(\n    prompt=prompt,\n    model=model,\n    tokenizer=tokenizer,\n    max_length=200,  # Maximum length of the generated sequence\n    num_return_sequences=1,  # Number of sequences to generate\n    temperature=0.7,  # Controls the randomness\n    top_k=50,  # Limits the next-word choices to the top-k highest probability words\n    top_p=0.95  # Nucleus sampling probability threshold\n)\n\n# Print the generated text\nfor i, text in enumerate(generated_texts):\n    print(f\"Generated Text {i + 1}:\\n{text}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-18T21:52:05.726221Z","iopub.execute_input":"2024-09-18T21:52:05.726918Z","iopub.status.idle":"2024-09-18T21:52:39.634244Z","shell.execute_reply.started":"2024-09-18T21:52:05.726875Z","shell.execute_reply":"2024-09-18T21:52:39.633243Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Generated Text 1:\nIl était une fois une fille qui lavait un pagne dans la riviere des l'aube. Elle était si belle, si gracieuse, qu'elle attira l'attention de tous les jeunes gens du village. Parmi eux, il y avait un jeune homme qui était amoureux d'elle depuis longtemps. Il s'appelait  Koffi. Koffi était un jeune homme courageux et généreux. Il avait tout fait pour gagner le cœur de la belle fille, mais elle ne semblait pas l'aimer. Elle le repoussait toujours, et lui disait qu'elle ne voulait pas d'un mari qui n'était pas riche. Koffi était pauvre, mais il était prêt à tout pour elle. Un jour, il décida de partir à la recherche d'un trésor. Il avait entendu dire qu'il y avait un trésor caché dans la forêt. Koffi était un excellent chasseur et un grand connaisseur des plantes médicinales\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"new_model = \"google/gemma-2-9b-it-finetune-africa-book\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:00:19.728117Z","iopub.execute_input":"2024-09-18T22:00:19.728537Z","iopub.status.idle":"2024-09-18T22:00:19.734657Z","shell.execute_reply.started":"2024-09-18T22:00:19.728500Z","shell.execute_reply":"2024-09-18T22:00:19.733559Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:00:21.476693Z","iopub.execute_input":"2024-09-18T22:00:21.477080Z","iopub.status.idle":"2024-09-18T22:00:23.353254Z","shell.execute_reply.started":"2024-09-18T22:00:21.477039Z","shell.execute_reply":"2024-09-18T22:00:23.352218Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}